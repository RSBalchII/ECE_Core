{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3310ef29",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.memory.neo4j_store import Neo4jStore\n",
    "\n",
    "# Setup logging and reproducible seed\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.random.seed(1234)\n",
    "\n",
    "print('Imports complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbd39d",
   "metadata": {},
   "source": [
    "## 2. Configure Environment, Paths & Backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))  # adjust as necessary\n",
    "NOTEBOOK_BACKUP_DIR = os.path.join(BASE_DIR, 'notebooks', 'backups')\n",
    "os.makedirs(NOTEBOOK_BACKUP_DIR, exist_ok=True)\n",
    "DRY_RUN = True  # set to False to apply changes\n",
    "BATCH_LIMIT = 50  # number of nodes to test in notebook runs (for safety)\n",
    "print(f'Backups will be stored in: {NOTEBOOK_BACKUP_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d6dba",
   "metadata": {},
   "source": [
    "## 3. Load Dataset(s) into a Pandas DataFrame (from DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def list_missing_app_id(limit=BATCH_LIMIT):\n",
    "    store = Neo4jStore()\n",
    "    await store.initialize()\n",
    "    if not store.neo4j_driver:\n",
    "        raise RuntimeError('Neo4j driver not available')\n",
    "    q = ('MATCH (m:Memory) WHERE (m.app_id IS NULL OR m.app_id = \n",
    " ) '\n",
    "         'RETURN elementId(m) as eid, m.metadata as metadata, m.content as content, m.created_at as created_at LIMIT $limit')\n",
    "    rows = await store.execute_cypher(q, {'limit': limit})\n",
    "    await store.close()\n",
    "    df = pd.DataFrame(rows) if rows else pd.DataFrame(columns=['eid','metadata','content','created_at'])\n",
    "    return df\n",
    "\n",
    "df = asyncio.run(list_missing_app_id())\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60b643",
   "metadata": {},
   "source": [
    "## 4. Quick Data Inspection and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_inspect(df: pd.DataFrame):\n",
    "    print('Total missing:', len(df))\n",
    "    # Parse metadata JSON column (it may be a dict already or a JSON string)\n",
    "    def parse_meta(m):\n",
    "        try:\n",
    "            if isinstance(m, str) and m.strip():\n",
    "                return json.loads(m)\n",
    "            elif isinstance(m, dict):\n",
    "                return m\n",
    "        except Exception:\n",
    "            return {}\n",
    "        return {}\n",
    "    df['parsed_meta'] = df['metadata'].apply(parse_meta)\n",
    "    df['meta_app_id'] = df['parsed_meta'].apply(lambda x: x.get('app_id'))\n",
    "    print(df['meta_app_id'].value_counts(dropna=False).head(10))\n",
    "    return df\n",
    "\n",
    "df = sample_inspect(df)\n",
    "df[['eid','meta_app_id','content']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983819d",
   "metadata": {},
   "source": [
    "## 5. Define app_id Validation Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_ID_REGEX = re.compile(r'^[0-9a-fA-F\\-]{36}$')  # UUID v4/5 canonical form\n",
    "\n",
    "def is_valid_app_id(aid):\n",
    "    if not aid or not isinstance(aid, str):\n",
    "        return False\n",
    "    return bool(APP_ID_REGEX.match(aid))\n",
    "\n",
    "# Quick check on our sample\n",
    "bad_count = df['meta_app_id'].apply(lambda x: not is_valid_app_id(x) if x else True).sum()\n",
    "print(f'Invalid or missing app_id values in sample: {bad_count}/{len(df)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f966a3",
   "metadata": {},
   "source": [
    "## 6. Detect Missing, Duplicate, and Invalid app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc55a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_issues(df):\n",
    "    df['is_valid'] = df['meta_app_id'].apply(lambda x: is_valid_app_id(x))\n",
    "    df['is_missing'] = df['meta_app_id'].isnull()\n",
    "    # Detect duplicates among provided app_id values\n",
    "    dup_mask = df.duplicated('meta_app_id', keep=False) & (df['meta_app_id'].notnull())\n",
    "    df['is_dup'] = dup_mask\n",
    "    return df\n",
    "\n",
    "df = detect_issues(df)\n",
    "df[['eid','meta_app_id','is_missing','is_valid','is_dup']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc5d85",
   "metadata": {},
   "source": [
    "## 7. Summarize Issues and Visualize Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e987b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(df):\n",
    "    total = len(df)\n",
    "    missing = df['is_missing'].sum()\n",
    "    invalid = (~df['is_valid']).sum()\n",
    "    dup = df['is_dup'].sum()\n",
    "    return {'total': total, 'missing': int(missing), 'invalid': int(invalid), 'dup': int(dup)}\n",
    "\n",
    "print(report(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc229e7e",
   "metadata": {},
   "source": [
    "## 8. Normalize and Clean app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8014e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_app_id(aid):\n",
    "    if not aid or not isinstance(aid, str):\n",
    "        return None\n",
    "    # Trim and lower-case the characters; preserve hyphens; ensure canonical uuid format is preserved\n",
    "    x = aid.strip()\n",
    "    # If it looks like uuid in any case, normalize to lowercase\n",
    "    if APP_ID_REGEX.match(x):\n",
    "        return x.lower()\n",
    "    # Try to remove any non-hex and hyphen chars that might be noise\n",
    "    cleaned = re.sub('[^0-9a-fA-F\\-]', '', x)\n",
    "    if APP_ID_REGEX.match(cleaned):\n",
    "        return cleaned.lower()\n",
    "    return None\n",
    "\n",
    "# Apply normalization\n",
    "df['normalized_app_id'] = df['meta_app_id'].apply(normalize_app_id)\n",
    "df['normalized_app_id'].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e0432",
   "metadata": {},
   "source": [
    "## 9. Map External IDs to Canonical app_id (lookup table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af723889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mapping(df, mapping):\n",
    "    # mapping is dict: external -> canonical\n",
    "    df['mapped_app_id'] = df['normalized_app_id'].map(mapping).fillna(df['normalized_app_id'])\n",
    "    return df\n",
    "\n",
    "# Example mapping: empty for now\n",
    "mapping = {}\n",
    "df = apply_mapping(df, mapping)\n",
    "df[['eid','normalized_app_id','mapped_app_id']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e068b8",
   "metadata": {},
   "source": [
    "## 10. Generate New app_id for Missing or Invalid Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_app_id(row):\n",
    "    # Prefer deterministic generation from metadata values if available\n",
    "    try:\n",
    "        parsed = row['parsed_meta'] if pd.notnull(row['parsed_meta']) else {}\n",
    "    except Exception:\n",
    "        parsed = {}\n",
    "    if parsed.get('app_id'):\n",
    "        return parsed['app_id']\n",
    "    if parsed.get('source') and parsed.get('chunk_index') is not None:\n",
    "        ns = uuid.UUID('f8bd0f6e-0c4c-4654-9201-12c4f2b4b5ef')\n",
    "        return str(uuid.uuid5(ns, f\n",
    "    content = row.get('content') or ''\n",
    "    ns = uuid.UUID('f8bd0f6e-0c4c-4654-9201-12c4f2b4b5ef')\n",
    "    return str(uuid.uuid5(ns, content[:4096]))\n",
    "\n",
    "# Apply only to rows missing or invalid mapped_app_id\n",
    "mask = (~df['mapped_app_id'].apply(is_valid_app_id)) | df['mapped_app_id'].isnull()\n",
    "df.loc[mask, 'new_app_id'] = df[mask].apply(generate_app_id, axis=1)\n",
    "df[['eid','meta_app_id','normalized_app_id','mapped_app_id','new_app_id']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2ff19",
   "metadata": {},
   "source": [
    "## 11. Apply Repairs with Dry Run and Change Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_change_log(df):\n",
    "    changes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        old = row['meta_app_id'] if row.get('meta_app_id') else None\n",
    "        new = row['new_app_id'] if row.get('new_app_id') else row.get('mapped_app_id')\n",
    "        if new and (old != new):\n",
    "            changes.append({'eid': row['eid'], 'old_app_id': old, 'new_app_id': new, 'reason': 'missing/invalid/dupe'})\n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "changes = build_change_log(df)\n",
    "print(changes.head(20))\n",
    "\n",
    "# If DRY_RUN, do not apply changes to DB. Otherwise, update nodes in Neo4j with new app_id and metadata\n",
    "async def apply_changes_to_db(changes_df, dry_run=True):\n",
    "    if changes_df.empty:\n",
    "        print('No changes to apply')\n",
    "        return\n",
    "    store = Neo4jStore()\n",
    "    await store.initialize()\n",
    "    applied = []\n",
    "    for _, r in changes_df.iterrows():\n",
    "        eid = r['eid']\n",
    "        new_app_id = r['new_app_id']\n",
    "        if not dry_run:\n",
    "            # Fetch existing metadata\n",
    "            q = 'MATCH (m:Memory) WHERE elementId(m) = $eid RETURN m.metadata as metadata'\n",
    "            res = await store.execute_cypher(q, {'eid': eid})\n",
    "            meta = {}\n",
    "            if res and res[0].get('metadata'):\n",
    "                try:\n",
    "                    meta = json.loads(res[0]['metadata']) if isinstance(res[0]['metadata'], str) else res[0]['metadata']\n",
    "                except Exception:\n",
    "                    meta = {}\n",
    "            meta['app_id'] = new_app_id\n",
    "            # Update node property and metadata field to include app_id\n",
    "            update_q = 'MATCH (m:Memory) WHERE elementId(m) = $eid SET m.app_id = $app_id, m.metadata = $meta RETURN elementId(m) as eid'\n",
    "            await store.execute_cypher(update_q, {'eid': eid, 'app_id': new_app_id, 'meta': json.dumps(meta)})\n",
    "            applied.append(eid)\n",
    "    await store.close()\n",
    "    print(f'Applied {len(applied)} updates')\n",
    "\n",
    "# Run as dry run here\n",
    "changes_df = changes.copy()\n",
    "changes_df['new_app_id'] = changes_df['new_app_id'].fillna(changes_df['new_app_id'])\n",
    "print('Preview of changes:')\n",
    "print(changes_df.head(20))\n",
    "\n",
    "# To apply changes, toggle dry_run to False and run:\n",
    "# asyncio.run(apply_changes_to_db(changes_df, dry_run=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afd80f",
   "metadata": {},
   "source": [
    "## 12. Persist Repairs and Create Versioned Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_original_df(df, backup_dir=NOTEBOOK_BACKUP_DIR):\n",
    "    if df.empty:\n",
    "        return None\n",
    "    outpath = os.path.join(backup_dir, f'missing_appid_backup_{pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")}.parquet')\n",
    "    df.to_parquet(outpath, index=False)\n",
    "    return outpath\n",
    "\n",
    "bk = backup_original_df(df)\n",
    "print('Backup saved to:', bk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d1fbe",
   "metadata": {},
   "source": [
    "## 13. Unit Tests and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b24b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic sanity tests (not using pytest in notebook)\n",
    "assert is_valid_app_id(str(uuid.uuid4())) == True\n",
    "assert normalize_app_id(' ' + str(uuid.uuid4()).upper() + ' ') is not None\n",
    "print('Basic tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcd10f",
   "metadata": {},
   "source": [
    "## 14. End-to-end Example: Fix a Sample Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo run: build the change log and run in dry-run mode, optionally apply\n",
    "changes_preview = changes.copy()\n",
    "print(changes_preview.head(20))\n",
    "# To apply changes in the real DB (on a small subset) set DRY_RUN=False and run the apply function\n",
    "# asyncio.run(apply_changes_to_db(changes_preview, dry_run=False))\n",
    "\n",
    "print('End of notebook')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
